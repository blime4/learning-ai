## Learning AI

æœ¬ä»“åº“åŒ…å« AI å­¦ä¹ ç¬”è®°ä¸ä»£ç ç¤ºä¾‹ï¼Œé‡ç‚¹å›´ç»• **llama.cpp** ç”Ÿæ€ã€‚

> ğŸ“– å®Œæ•´é˜…è¯»é¡ºåº PDFï¼š[`llama_cpp_reading_order.pdf`](./llama_cpp_reading_order.pdf)
>
> ğŸ—ºï¸ äº¤äº’å¼ Roadmapï¼ˆå¸¦è¿›åº¦è¿½è¸ªï¼‰ï¼š[æ‰“å¼€ Roadmap](https://blime4.github.io/learning-ai/roadmap.html)

---

## ğŸ—ºï¸ llama.cpp å­¦ä¹ è·¯çº¿å›¾

è¿›åº¦è¿½è¸ªï¼šåœ¨ GitHub ä¸Šç›´æ¥ç¼–è¾‘æ­¤æ–‡ä»¶ï¼Œå‹¾é€‰ `[ ]` â†’ `[x]` å³å¯è®°å½•å­¦ä¹ è¿›åº¦ã€‚

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ä¸ç¯å¢ƒæ­å»º

- [ ] **1.** [`fundamentals/ggml/README.md`](./fundamentals/ggml/README.md) â€” GGML æ˜¯ llama.cpp çš„åº•å±‚å¼ é‡åº“ï¼Œå…ˆç†è§£ tensorã€backendã€è®¡ç®—å›¾ç­‰åŸºæœ¬æ¦‚å¿µ
- [ ] **2.** [`fundamentals/llama.cpp/README.md`](./fundamentals/llama.cpp/README.md) â€” é¡¹ç›®å…¥å£ï¼šå­æ¨¡å—é…ç½®ã€ç¼–è¯‘æ–¹å¼ã€GDB è°ƒè¯•ã€CUDA æ„å»º
- [ ] **3.** [`notes/llama.cpp/debugging.md`](./notes/llama.cpp/debugging.md) â€” è°ƒè¯•æŠ€å·§ï¼Œåç»­é˜…è¯»æºç ç¬”è®°æ—¶ä¼šé¢‘ç¹ç”¨åˆ°

### ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒæ¦‚å¿µï¼ˆæ¨ç†æµæ°´çº¿ï¼‰

- [ ] **4.** [`fundamentals/llama.cpp/src/tokenize.cpp`](./fundamentals/llama.cpp/src/tokenize.cpp) / [`notes/llama.cpp/llama-vocab-notes.md`](./notes/llama.cpp/llama-vocab-notes.md) â€” åˆ†è¯å™¨ â€” æ¨ç†çš„ç¬¬ä¸€æ­¥
- [ ] **5.** [`fundamentals/llama.cpp/src/simple-prompt.cpp`](./fundamentals/llama.cpp/src/simple-prompt.cpp) â€” æœ€ç®€å•çš„æ¨ç†ç¤ºä¾‹ï¼Œç†è§£æ•´ä½“è°ƒç”¨æµç¨‹
- [ ] **6.** [`notes/llama.cpp/llama-batch.md`](./notes/llama.cpp/llama-batch.md) â€” llama_batch å’Œ llama_ubatch ç»“æ„ï¼Œtoken å¦‚ä½•ç»„ç»‡é€å…¥æ¨¡å‹
- [ ] **7.** [`notes/llama.cpp/process_ubatch.md`](./notes/llama.cpp/process_ubatch.md) â€” micro-batch å¤„ç†ç»†èŠ‚
- [ ] **8.** [`notes/llama.cpp/kv-cache.md`](./notes/llama.cpp/kv-cache.md) â€” KV ç¼“å­˜æœºåˆ¶ â€” æ¨ç†åŠ é€Ÿçš„æ ¸å¿ƒ
- [ ] **9.** [`fundamentals/llama.cpp/src/kv-cache.cpp`](./fundamentals/llama.cpp/src/kv-cache.cpp) â€” KV ç¼“å­˜çš„ä»£ç å®è·µ
- [ ] **10.** [`notes/llama.cpp/graph-inputs.md`](./notes/llama.cpp/graph-inputs.md) â€” è®¡ç®—å›¾è¾“å…¥çš„æ„å»º
- [ ] **11.** [`notes/llama.cpp/gpu-sampling.md`](./notes/llama.cpp/gpu-sampling.md) â€” GPU ä¸Šçš„é‡‡æ ·å®ç°ï¼ˆtemperatureã€top-kã€top-p ç­‰ï¼‰
- [ ] **12.** [`notes/llama.cpp/output_ids.md`](./notes/llama.cpp/output_ids.md) â€” è¾“å‡º token ID çš„å¤„ç†
- [ ] **13.** [`fundamentals/llama.cpp/src/simple-prompt-multi.cpp`](./fundamentals/llama.cpp/src/simple-prompt-multi.cpp) â€” å¤š prompt æ‰¹å¤„ç†ç¤ºä¾‹

### ç¬¬ä¸‰é˜¶æ®µï¼šGPU åŠ é€Ÿä¸åç«¯

- [ ] **14.** [`notes/llama.cpp/cuda.md`](./notes/llama.cpp/cuda.md) â€” CUDA åç«¯åŠ è½½æœºåˆ¶ï¼ˆggml_backend_load_allï¼‰
- [ ] **15.** [`notes/llama.cpp/cuda-mul-mat.md`](./notes/llama.cpp/cuda-mul-mat.md) â€” CUDA çŸ©é˜µä¹˜æ³•å®ç°
- [ ] **16.** [`notes/llama.cpp/cuda-fp16-release-build-issue.md`](./notes/llama.cpp/cuda-fp16-release-build-issue.md) â€” FP16 æ„å»ºé—®é¢˜è®°å½•
- [ ] **17.** [`fundamentals/ggml/src/llama-att-softmax.cpp`](./fundamentals/ggml/src/llama-att-softmax.cpp) â€” attention softmax çš„ GGML å®ç°
- [ ] **18.** [`notes/llama.cpp/flash-attn-misalignment-issue.md`](./notes/llama.cpp/flash-attn-misalignment-issue.md) â€” Flash Attention å¯¹é½é—®é¢˜
- [ ] **19.** [`notes/llama.cpp/macosx.md`](./notes/llama.cpp/macosx.md) â€” macOS (Metal) å¹³å°ç›¸å…³
- [ ] **20.** [`notes/llama.cpp/ggml-threadpool-macos-issue.md`](./notes/llama.cpp/ggml-threadpool-macos-issue.md) â€” çº¿ç¨‹æ± é—®é¢˜

### ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹è½¬æ¢ä¸é‡åŒ–

- [ ] **21.** [`notes/llama.cpp/convert.md`](./notes/llama.cpp/convert.md) â€” convert_hf_to_gguf.py æµç¨‹è§£æ
- [ ] **22.** [`notes/llama.cpp/quantization.md`](./notes/llama.cpp/quantization.md) â€” é‡åŒ–åŸç†ä¸ QAT é‡åŒ–
- [ ] **23.** [`notes/llama.cpp/convert-dequantize.md`](./notes/llama.cpp/convert-dequantize.md) â€” åé‡åŒ–è¿‡ç¨‹
- [ ] **24.** [`notes/llama.cpp/devstral2-conversion.md`](./notes/llama.cpp/devstral2-conversion.md) â€” Devstral2 æ¨¡å‹è½¬æ¢å®ä¾‹
- [ ] **25.** [`notes/llama.cpp/convert-mamba-issue.md`](./notes/llama.cpp/convert-mamba-issue.md) â€” Mamba æ¨¡å‹è½¬æ¢é—®é¢˜
- [ ] **26.** [`notes/llama.cpp/gemma-bos-issue.md`](./notes/llama.cpp/gemma-bos-issue.md) â€” Gemma BOS token é—®é¢˜

### ç¬¬äº”é˜¶æ®µï¼šEmbeddings

- [ ] **27.** [`fundamentals/llama.cpp/src/embeddings.cpp`](./fundamentals/llama.cpp/src/embeddings.cpp) â€” embedding ç”Ÿæˆä»£ç 
- [ ] **28.** [`notes/llama.cpp/embeddings-presets.md`](./notes/llama.cpp/embeddings-presets.md) â€” embedding é¢„è®¾é…ç½®

### ç¬¬å…­é˜¶æ®µï¼šServer ä¸éƒ¨ç½²

- [ ] **29.** [`notes/llama.cpp/llama-server.md`](./notes/llama.cpp/llama-server.md) â€” server å¯åŠ¨ä¸ API è°ƒç”¨
- [ ] **30.** [`notes/llama.cpp/server-checkpoints.md`](./notes/llama.cpp/server-checkpoints.md) â€” checkpoint ç®¡ç†
- [ ] **31.** [`notes/llama.cpp/server-logprob-issue.md`](./notes/llama.cpp/server-logprob-issue.md) â€” log probability é—®é¢˜
- [ ] **32.** [`notes/llama.cpp/server-unit-tests.md`](./notes/llama.cpp/server-unit-tests.md) â€” server æµ‹è¯•
- [ ] **33.** [`notes/llama.cpp/llama-perplexity.md`](./notes/llama.cpp/llama-perplexity.md) â€” å›°æƒ‘åº¦è®¡ç®—
- [ ] **34.** [`notes/llama.cpp/tests.md`](./notes/llama.cpp/tests.md) â€” æµ‹è¯•æ¡†æ¶
- [ ] **35.** [`notes/llama.cpp/sbatch.md`](./notes/llama.cpp/sbatch.md) â€” SLURM æ‰¹é‡æäº¤

### ç¬¬ä¸ƒé˜¶æ®µï¼šFinetuning

- [ ] **36.** [`fundamentals/llama.cpp/README.md`](./fundamentals/llama.cpp/README.md) â€” LoRA å¾®è°ƒã€Shakespeare æ•°æ®é›†ã€chat æ ¼å¼è®­ç»ƒ

### ç¬¬å…«é˜¶æ®µï¼šå¤šæ¨¡æ€ä¸ç‰¹æ®Šæ¨¡å‹

- [ ] **37.** [`notes/llama.cpp/llama-3-2-vision.md`](./notes/llama.cpp/llama-3-2-vision.md) â€” Llama 3.2 è§†è§‰æ¨¡å‹
- [ ] **38.** [`notes/llama.cpp/qwen-2.5VL-3B-instruct.md`](./notes/llama.cpp/qwen-2.5VL-3B-instruct.md) â€” Qwen è§†è§‰æ¨¡å‹
- [ ] **39.** [`notes/llama.cpp/vision-model-issue.md`](./notes/llama.cpp/vision-model-issue.md) â€” è§†è§‰æ¨¡å‹é—®é¢˜
- [ ] **40.** [`fundamentals/image-processing/src/mllama.cpp`](./fundamentals/image-processing/src/mllama.cpp) â€” Llama è§†è§‰æ¨¡å‹å®ç°
- [ ] **41.** [`notes/llama.cpp/tts.md`](./notes/llama.cpp/tts.md) â€” TTS é›†æˆ

### ç¬¬ä¹é˜¶æ®µï¼šAgent ä¸ä¸Šå±‚åº”ç”¨

- [ ] **42.** [`agents/llama-cpp-agent/README.md`](./agents/llama-cpp-agent/README.md) â€” åŸºäº WASM çš„ agent æ¡†æ¶
- [ ] **43.** [`agents/llama-cpp-agent/agent/src/main.rs`](./agents/llama-cpp-agent/agent/src/main.rs) / [`agent.rs`](./agents/llama-cpp-agent/agent/src/agent.rs) / [`tool.rs`](./agents/llama-cpp-agent/agent/src/tool.rs) â€” Rust agent å®ç°

### ç¬¬åé˜¶æ®µï¼šå…¶ä»–è¯­è¨€ç»‘å®šä¸é›†æˆ

- [ ] **44.** [`fundamentals/python/src/llama-chat-format.py`](./fundamentals/python/src/llama-chat-format.py) â€” Python chat æ ¼å¼å¤„ç†
- [ ] **45.** [`fundamentals/rust/llm-chains-llama-example/README.md`](./fundamentals/rust/llm-chains-llama-example/README.md) â€” Rust LLM chains + Llama
- [ ] **46.** [`fundamentals/rust/llm-chains-chat-demo/src/main-llama.rs`](./fundamentals/rust/llm-chains-chat-demo/src/main-llama.rs) â€” Rust chat demo

### è¡¥å……ï¼šIssue ç¬”è®°ï¼ˆæŒ‰éœ€æŸ¥é˜…ï¼‰

- [ ] **A1.** [`notes/llama.cpp/sched-issue.md`](./notes/llama.cpp/sched-issue.md) â€” è°ƒåº¦é—®é¢˜
- [ ] **A2.** [`notes/llama.cpp/update_chat_msg-issue.md`](./notes/llama.cpp/update_chat_msg-issue.md) â€” chat æ¶ˆæ¯æ›´æ–°é—®é¢˜

---

### Topics

* [Tokenization](./notes/tokenization/README.md)
* [Architectures](./notes/architectures/README.md)
* [GGML](./notes/ggml.md)
* [Llama.cpp](./notes/llama.md)
* [Position Embeddings](./notes/position-embeddings)
* [GPUs](./gpu/README.md)
* [Vector Databases](./notes/vector-databases.md)
* [Vision](./notes/vision)

### Examples/Exploration code

* [GGML](./fundamentals/ggml) GGML C library exploration code
* [Llama.cpp](fundamentals/llama.cpp) Llama.cpp library exploration code
* [GPU](gpu/README.md) CUDA, Kompute, Metal, OpenCL, ROCm, and Vulkan exploration code
* [Embeddings](./embeddings) Word embeddings examples in Rust and Python
* [Huggingface API](./hugging-face/python) Huggingface API example written in Python
* [Qdrant Vector Database](./vector-databases/qdrant) Examples in Python and Rust
* [LanceDB Vector Database](./vector-databases/lancedb) Examples in Python and Rust
